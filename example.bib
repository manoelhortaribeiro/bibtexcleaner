@book{saggion_automatic_2017,
	address = {San Rafael, Kalifornien},
	title = {Automatic {Text} {Simplification}},
	isbn = {978-1-62705-868-1},
	language = {English},
	publisher = {Morgan \& Claypool Publishers},
	author = {Saggion, Horacio},
	editor = {Hirst, Graeme},
	month = apr,
	year = {2017},
	note = {BOOK},
}

@misc{noauthor_rdf_nodate,
	title = {{RDF} {Semantics}},
	url = {https://www.w3.org/TR/2004/REC-rdf-mt-20040210/},
	urldate = {2022-01-18},
	note = {MISC},
}

@article{bengio_estimating_2013,
	title = {Estimating or {Propagating} {Gradients} {Through} {Stochastic} {Neurons} for {Conditional} {Computation}},
	url = {http://arxiv.org/abs/1308.3432},
	abstract = {Stochastic neurons and hard non-linearities can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic or non-smooth neurons? I.e., can we "back-propagate" through these stochastic neurons? We examine this question, existing approaches, and compare four families of solutions, applicable in different settings. One of them is the minimum variance unbiased gradient estimator for stochatic binary neurons (a special case of the REINFORCE algorithm). A second approach, introduced here, decomposes the operation of a binary stochastic neuron into a stochastic binary part and a smooth differentiable part, which approximates the expected effect of the pure stochatic binary neuron to first order. A third approach involves the injection of additive or multiplicative noise in a computational graph that is otherwise differentiable. A fourth approach heuristically copies the gradient with respect to the stochastic output directly as an estimator of the gradient with respect to the sigmoid argument (we call this the straight-through estimator). To explore a context where these estimators are useful, we consider a small-scale version of \{{\textbackslash}em conditional computation\}, where sparse stochastic units form a distributed representation of gaters that can turn off in combinatorially many ways large chunks of the computation performed in the rest of the neural network. In this case, it is important that the gating units produce an actual 0 most of the time. The resulting sparsity can be potentially be exploited to greatly reduce the computational cost of large deep networks for which conditional computation would be useful.},
	urldate = {2021-12-25},
	journal = {arXiv:1308.3432 [cs]},
	author = {Bengio, Yoshua and LÃ©onard, Nicholas and Courville, Aaron},
	month = aug,
	year = {2013},
	note = {ONLY},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{maddison_concrete_2017,
	title = {The {Concrete} {Distribution}: {A} {Continuous} {Relaxation} of {Discrete} {Random} {Variables}},
	shorttitle = {The {Concrete} {Distribution}},
	url = {http://arxiv.org/abs/1611.00712},
	abstract = {The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce Concrete random variables---continuous relaxations of discrete random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.},
	urldate = {2021-12-25},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Maddison, Chris J. and Mnih, Andriy and Teh, Yee Whye},
	month = mar,
	year = {2017},
	note = {ICLR},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}